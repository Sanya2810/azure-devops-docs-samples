# get storage account name, container name from input
dbutils.widgets.text("input", "","")
datafile = dbutils.widgets.get("input")
storage_account_name = getArgument("storage_account_name")
storage_container_name = getArgument("storage_container_name")

# mount the blob storage that represents the target data
mount_point = "/mnt/prepared"
if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()): 
  dbutils.fs.mount(
    source = "wasbs://"+storage_container_name+"@"+storage_account_name+".blob.core.windows.net",
    mount_point = mount_point,
    extra_configs = {"fs.azure.account.key."+storage_account_name+".blob.core.windows.net":dbutils.secrets.get(scope = "testscope", key = "StorageKey")})

# read the files with column information
data = spark.read.format("csv")\
.option("inferSchema", 'true')\
.option("header",'true') \
.load(mount_point+"/"+datafile)

# transformation
import pandas as pd
from datetime import datetime
from pyspark.sql.functions import *
# Final transformation
new_names = ['Unnamed: 0', 'transaction_time', 'credit_card_number', 'merchant',
       'category', 'amount(usd)', 'first', 'last', 'gender', 'street', 'city',
       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob',
       'transaction_id', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud']
data = data.toDF(*new_names)
# Add column of age from dob
data = data.withColumn("dob", to_timestamp("dob", 'yyyy-MM-dd'))
data = data.withColumn('age', floor(datediff(current_date(), data.dob)/365.25))

# Add column of hour of the day
data = data.withColumn("transaction_time", to_timestamp("transaction_time", 'yyyy-MM-dd HH:mm:ss'))
data = data.withColumn('hour', hour(data.transaction_time))

# select important features only
features = ['transaction_id', 'category', 'amount(usd)', 'merchant', 'job','hour','age','is_fraud']
df = data[features]

# save the transformed file as "transformed.csv"
filepath_to_save = '/dbfs' + mount_point + '/transformed.csv'
df.toPandas().to_csv(filepath_to_save,mode = 'w', index=False)
